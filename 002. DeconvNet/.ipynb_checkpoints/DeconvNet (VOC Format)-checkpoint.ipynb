{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T07:28:36.660329Z",
     "start_time": "2021-01-13T07:28:36.657811Z"
    }
   },
   "source": [
    "## Package load "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:08:37.534269Z",
     "start_time": "2021-02-22T12:08:35.288794Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.4.0\n",
      "GPU 사용 가능 여부: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 하이퍼파라미터 세팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:08:37.859263Z",
     "start_time": "2021-02-22T12:08:37.856801Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 16                                      # Mini-batch size    \n",
    "num_epochs = 50\n",
    "learning_rate = 0.0003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 전처리 함수 정의 (VOC2007 Dataset)\n",
    "\n",
    "- Download VOC2007 dataset from link in references. File should have name VOCtrainval_06-Nov-2007.tar and weight approx 460MB. Extract VOC2007 folder and modify path below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:09:02.562263Z",
     "start_time": "2021-02-22T12:09:02.559766Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset v3\n",
    "\n",
    "num_classes = 21                                        # background, airplane, ..., border\n",
    "data_root = \"../data/VOC2007\"                           # Dataset location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:09:46.796248Z",
     "start_time": "2021-02-22T12:09:46.792245Z"
    }
   },
   "outputs": [],
   "source": [
    "# for reference, not used in this notebook\n",
    "voc_classes = ('background',  # always index 0\n",
    "               'aeroplane', 'bicycle', 'bird', 'boat',  # indices 1, 2, 3, 4\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',  #         5, ...\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor',   #         ..., 21\n",
    "               )                                # but border has index 255 (!) / (불필요시 수정 필요)\n",
    "\n",
    "assert num_classes == len(voc_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:09:47.817745Z",
     "start_time": "2021-02-22T12:09:47.814745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:09:50.089426Z",
     "start_time": "2021-02-22T12:09:50.079927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Class below reads segmentation dataset in VOC2007 compatible format.\n",
    "\n",
    "class PascalVOCDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Pascal VOC2007 or compatible dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes, list_file, img_dir, mask_dir, transform=None):\n",
    "        self.num_classes = num_classes\n",
    "        self.images = open(list_file, \"rt\").read().split(\"\\n\")[:-1]\n",
    "        self.transform = transform\n",
    "        self.img_extension = \".jpg\"\n",
    "        self.mask_extension = \".png\"\n",
    "        self.image_root_dir = img_dir\n",
    "        self.mask_root_dir = mask_dir\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.images[index]\n",
    "        image_path = os.path.join(self.image_root_dir, name + self.img_extension)\n",
    "        mask_path = os.path.join(self.mask_root_dir, name + self.mask_extension)\n",
    "\n",
    "        image = self.load_image(path=image_path)\n",
    "        gt_mask = self.load_mask(path=mask_path)\n",
    "\n",
    "        return torch.FloatTensor(image), torch.LongTensor(gt_mask)\n",
    "\n",
    "    \n",
    "    def load_image(self, path=None):\n",
    "        raw_image = PIL.Image.open(path)\n",
    "        raw_image = np.transpose(raw_image.resize((224, 224)), (2,1,0))\n",
    "        imx_t = np.array(raw_image, dtype=np.float32)/255.0\n",
    "        return imx_t\n",
    "\n",
    "    \n",
    "    def load_mask(self, path=None):\n",
    "        raw_image = PIL.Image.open(path)\n",
    "        raw_image = raw_image.resize((224, 224))\n",
    "        imx_t = np.array(raw_image)\n",
    "        imx_t[imx_t==255] = self.num_classes-1        # convert VOC border into last class\n",
    "        return imx_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 정의 및 DataLoader 할당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:09:51.594389Z",
     "start_time": "2021-02-22T12:09:51.591388Z"
    }
   },
   "outputs": [],
   "source": [
    "train_path = os.path.join(data_root, 'ImageSets/Segmentation/train.txt')\n",
    "val_path = os.path.join(data_root, 'ImageSets/Segmentation/val.txt')\n",
    "\n",
    "img_dir = os.path.join(data_root, \"JPEGImages\")\n",
    "mask_dir = os.path.join(data_root, \"SegmentationClass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Create train and validation datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:09:52.879837Z",
     "start_time": "2021-02-22T12:09:52.871337Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/VOC2007\\\\ImageSets/Segmentation/train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-b180d899a787>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m                                  \u001b[0mlist_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                                  \u001b[0mimg_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                                  mask_dir=mask_dir)\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m val_dataset = PascalVOCDataset(num_classes = num_classes, \n",
      "\u001b[1;32m<ipython-input-17-46782afcd112>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, num_classes, list_file, img_dir, mask_dir, transform)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rt\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_extension\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\".jpg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/VOC2007\\\\ImageSets/Segmentation/train.txt'"
     ]
    }
   ],
   "source": [
    "train_dataset = PascalVOCDataset(num_classes = num_classes, \n",
    "                                 list_file = train_path,\n",
    "                                 img_dir = img_dir, \n",
    "                                 mask_dir=mask_dir)\n",
    "\n",
    "val_dataset = PascalVOCDataset(num_classes = num_classes, \n",
    "                               list_file=val_path,\n",
    "                               img_dir=img_dir, \n",
    "                               mask_dir=mask_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T19:33:16.243021Z",
     "start_time": "2021-01-14T19:33:16.239524Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
    "                                           batch_size = batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, \n",
    "                                         batch_size = batch_size,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T19:33:16.503022Z",
     "start_time": "2021-01-14T19:33:16.495023Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Train Dataset:')\n",
    "print('  length:', len(train_dataset))\n",
    "print(\"------------------------\")\n",
    "print('Validation Dataset:')\n",
    "print('  length:', len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 샘플 시각화 (Show example image and mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T19:33:17.360578Z",
     "start_time": "2021-01-14T19:33:17.150080Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "image, mask = train_dataset[10]\n",
    "image.transpose_(0, 2)\n",
    "\n",
    "print('image shape:', list(image.shape))\n",
    "print('mask shape: ', list(mask.shape))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(8,4))\n",
    "ax1.imshow(image)\n",
    "ax2.imshow(mask)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T19:33:17.530997Z",
     "start_time": "2021-01-14T19:33:17.523498Z"
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "mask array에 들어있는 unique value 로 각각 의미하는 label은 아래와 같음\n",
    "0 : background\n",
    "5 : bottle\n",
    "20 : tvmonitor\n",
    "21 : border\n",
    "'''\n",
    "\n",
    "set(mask.numpy().reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 네트워크 설계 (Pretrained 되지 않은 모델 사용) \n",
    "- DeconvNet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://drive.google.com/uc?export=view&id=1yzT-L_cXlbgJLnruD4IA1K2riHZE54ZA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:10:09.678663Z",
     "start_time": "2021-02-22T12:10:09.647662Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DeconvNet(nn.Module):\n",
    "    def __init__(self, num_classes=21):\n",
    "        super(DeconvNet, self).__init__()\n",
    "        self.relu    = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # conv1 \n",
    "        self.conv1_1 = nn.Conv2d(3, 64, 3, padding=1)\n",
    "        self.relu1_1 = nn.ReLU(inplace=True)\n",
    "        self.bn1_1 = nn.BatchNorm2d(64)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, 3, padding=1)\n",
    "        self.relu1_2 = nn.ReLU(inplace=True)\n",
    "        self.bn1_2 = nn.BatchNorm2d(64)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True, return_indices=True) \n",
    "\n",
    "        # conv2 \n",
    "        self.conv2_1 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.relu2_1 = nn.ReLU(inplace=True)\n",
    "        self.bn2_1 = nn.BatchNorm2d(128)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "        self.relu2_2 = nn.ReLU(inplace=True)\n",
    "        self.bn2_2 = nn.BatchNorm2d(128)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True, return_indices=True) \n",
    "\n",
    "        # conv3\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.relu3_1 = nn.ReLU(inplace=True)\n",
    "        self.bn3_1 = nn.BatchNorm2d(256)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.relu3_2 = nn.ReLU(inplace=True)\n",
    "        self.bn3_2 = nn.BatchNorm2d(256)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, 3, padding=1)\n",
    "        self.relu3_3 = nn.ReLU(inplace=True)\n",
    "        self.bn3_3 = nn.BatchNorm2d(256)\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True, return_indices=True) \n",
    "\n",
    "        # conv4\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.relu4_1 = nn.ReLU(inplace=True)\n",
    "        self.bn4_1 = nn.BatchNorm2d(512)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu4_2 = nn.ReLU(inplace=True)\n",
    "        self.bn4_2 = nn.BatchNorm2d(512)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu4_3 = nn.ReLU(inplace=True)\n",
    "        self.bn4_3 = nn.BatchNorm2d(512)\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True, return_indices=True) \n",
    "\n",
    "        # conv5\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_1 = nn.ReLU(inplace=True)\n",
    "        self.bn5_1 = nn.BatchNorm2d(512)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_2 = nn.ReLU(inplace=True)\n",
    "        self.bn5_2 = nn.BatchNorm2d(512)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, 3, padding=1)\n",
    "        self.relu5_3 = nn.ReLU(inplace=True)\n",
    "        self.bn5_3 = nn.BatchNorm2d(512)\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True, return_indices=True) \n",
    "        \n",
    "        # fc1\n",
    "        self.fc6 = nn.Conv2d(512, 4096, 1)\n",
    "        self.relu6 = nn.ReLU(inplace=True)\n",
    "        self.bn6 = nn.BatchNorm2d(4096)\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "\n",
    "        # fc2\n",
    "        self.fc7 = nn.Conv2d(4096, 4096, 1)\n",
    "        self.relu7 = nn.ReLU(inplace=True)\n",
    "        self.bn7 = nn.BatchNorm2d(4096)\n",
    "        self.drop7 = nn.Dropout2d()\n",
    "\n",
    "        # Deconv\n",
    "        self.deconv6 = nn.ConvTranspose2d(4096, 512, kernel_size=7, padding=3)\n",
    "        self.debn6 = nn.BatchNorm2d(512)\n",
    "        \n",
    "        # Deconv5\n",
    "        self.unpool5 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv5_3 = nn.ConvTranspose2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.debn5_3 = nn.BatchNorm2d(512)\n",
    "        self.deconv5_2 = nn.ConvTranspose2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.debn5_2 = nn.BatchNorm2d(512)\n",
    "        self.deconv5_1 = nn.ConvTranspose2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.debn5_1 = nn.BatchNorm2d(512)\n",
    "\n",
    "        # Deconv4\n",
    "        self.unpool4 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv4_3 = nn.ConvTranspose2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.debn4_3 = nn.BatchNorm2d(512)\n",
    "        self.deconv4_2 = nn.ConvTranspose2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.debn4_2 = nn.BatchNorm2d(512)\n",
    "        self.deconv4_1 = nn.ConvTranspose2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.debn4_1 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Deconv3\n",
    "        self.unpool3 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv3_3 = nn.ConvTranspose2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.debn3_3 = nn.BatchNorm2d(256)\n",
    "        self.deconv3_2 = nn.ConvTranspose2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.debn3_2 = nn.BatchNorm2d(256)\n",
    "        self.deconv3_1 = nn.ConvTranspose2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.debn3_1 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        # Deconv2\n",
    "        self.unpool2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv2_2 = nn.ConvTranspose2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.debn2_2 = nn.BatchNorm2d(128)\n",
    "        self.deconv2_1 = nn.ConvTranspose2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.debn2_1 = nn.BatchNorm2d(64)\n",
    "        \n",
    "        # Deconv1\n",
    "        self.unpool1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.deconv1_2 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.debn1_2 = nn.BatchNorm2d(64)\n",
    "        self.deconv1_1 = nn.ConvTranspose2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.debn1_1 = nn.BatchNorm2d(64)\n",
    "        self.score_fr = nn.Conv2d(64, num_classes, kernel_size = 1)\n",
    "        \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "                # xavier_uniform은 bias에 대해서는 제공하지 않음 \n",
    "                # ValueError: Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.relu1_1(self.bn1_1(self.conv1_1(x)))\n",
    "        h = self.relu1_2(self.bn1_2(self.conv1_2(h)))\n",
    "        h, pool1_indices = self.pool1(h)\n",
    "        \n",
    "        h = self.relu2_1(self.bn2_1(self.conv2_1(h)))\n",
    "        h = self.relu2_2(self.bn2_2(self.conv2_2(h)))\n",
    "        h, pool2_indices = self.pool2(h)\n",
    "        \n",
    "        h = self.relu3_1(self.bn3_1(self.conv3_1(h)))\n",
    "        h = self.relu3_2(self.bn3_2(self.conv3_2(h)))\n",
    "        h = self.relu3_3(self.bn3_3(self.conv3_3(h)))\n",
    "        h, pool3_indices = self.pool3(h)\n",
    "        \n",
    "        h = self.relu4_1(self.bn4_1(self.conv4_1(h)))\n",
    "        h = self.relu4_2(self.bn4_2(self.conv4_2(h)))\n",
    "        h = self.relu4_3(self.bn4_3(self.conv4_3(h)))\n",
    "        h, pool4_indices = self.pool4(h)\n",
    "        \n",
    "        h = self.relu5_1(self.bn5_1(self.conv5_1(h)))\n",
    "        h = self.relu5_2(self.bn5_2(self.conv5_2(h)))\n",
    "        h = self.relu5_3(self.bn5_3(self.conv5_3(h)))\n",
    "        h, pool5_indices = self.pool5(h)\n",
    "        \n",
    "        h = self.relu6(self.bn6(self.fc6(h)))\n",
    "        h = self.drop6(h)\n",
    "\n",
    "        h = self.relu7(self.bn7(self.fc7(h)))\n",
    "        h = self.drop7(h)\n",
    "        \n",
    "        h = self.debn6(self.deconv6(h))\n",
    "        \n",
    "        h = self.unpool5(h, pool5_indices)\n",
    "        h = self.debn5_3(self.deconv5_3(h))\n",
    "        h = self.debn5_2(self.deconv5_2(h))\n",
    "        h = self.debn5_1(self.deconv5_1(h))\n",
    "        \n",
    "        h = self.unpool4(h, pool4_indices)\n",
    "        h = self.debn4_3(self.deconv4_3(h))\n",
    "        h = self.debn4_2(self.deconv4_2(h))\n",
    "        h = self.debn4_1(self.deconv4_1(h))\n",
    "        \n",
    "        h = self.unpool3(h, pool3_indices)\n",
    "        h = self.debn3_3(self.deconv3_3(h))\n",
    "        h = self.debn3_2(self.deconv3_2(h))\n",
    "        h = self.debn3_1(self.deconv3_1(h))\n",
    "        \n",
    "        h = self.unpool2(h, pool2_indices)\n",
    "        h = self.debn2_2(self.deconv2_2(h))\n",
    "        h = self.debn2_1(self.deconv2_1(h))\n",
    "        \n",
    "        h = self.unpool1(h, pool1_indices)\n",
    "        h = self.debn1_2(self.deconv1_2(h))\n",
    "        h = self.debn1_1(self.deconv1_1(h))\n",
    "        h = self.score_fr(h)        \n",
    "        return torch.sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-22T12:10:19.107145Z",
     "start_time": "2021-02-22T12:10:17.367675Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  torch.Size([1, 3, 224, 224])\n",
      "output shape :  torch.Size([1, 21, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n",
    "\n",
    "model = DeconvNet(num_classes=21)\n",
    "x = torch.randn([1, 3, 224, 224])\n",
    "print(\"input shape : \", x.shape)\n",
    "out = model(x)\n",
    "print(\"output shape : \", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가독성 있는 코드로 변경 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class DeconvNet(nn.Module):\n",
    "    def __init__(self, num_classes=21):\n",
    "        super(DeconvNet, self).__init__()\n",
    "        def CBR(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "            layers = []\n",
    "            layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding)]\n",
    "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            layers += [nn.ReLU()]\n",
    "\n",
    "            cbr = nn.Sequential(*layers)\n",
    "            return cbr\n",
    "        \n",
    "        def DCB(in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "            layers = []\n",
    "            layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
    "                                 kernel_size=kernel_size, stride=stride, padding=padding)]\n",
    "            layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
    "            cbr = nn.Sequential(*layers)\n",
    "            return cbr\n",
    "        \n",
    "        # conv1 \n",
    "        self.cbr1_1 = CBR(3, 64, 3, 1, 1)\n",
    "        self.cbr1_2 = CBR(64, 64, 3, 1, 1)\n",
    "        self.pool1 = nn.MaxPool2d(2, stride=2, ceil_mode=True, return_indices=True) \n",
    "\n",
    "        # conv2 \n",
    "        self.cbr2_1 = CBR(64, 128, 3, 1, 1)\n",
    "        self.cbr2_2 = CBR(128, 128, 3, 1, 1)\n",
    "        self.pool2 = nn.MaxPool2d(2, stride=2, ceil_mode=True, return_indices=True) \n",
    "\n",
    "        # conv3\n",
    "        self.cbr3_1 = CBR(128, 256, 3, 1, 1)\n",
    "        self.cbr3_2 = CBR(256, 256, 3, 1, 1)\n",
    "        self.cbr3_3 = CBR(256, 256, 3, 1, 1)\n",
    "        self.pool3 = nn.MaxPool2d(2, stride=2, ceil_mode=True, return_indices=True) \n",
    "\n",
    "        # conv4\n",
    "        self.cbr4_1 = CBR(256, 512, 3, 1, 1)\n",
    "        self.cbr4_2 = CBR(512, 512, 3, 1, 1)\n",
    "        self.cbr4_3 = CBR(512, 512, 3, 1, 1)\n",
    "        self.pool4 = nn.MaxPool2d(2, stride=2, ceil_mode=True, return_indices=True) \n",
    "\n",
    "        # conv5\n",
    "        self.cbr5_1 = CBR(512, 512, 3, 1, 1)\n",
    "        self.cbr5_2 = CBR(512, 512, 3, 1, 1)\n",
    "        self.cbr5_3 = CBR(512, 512, 3, 1, 1)\n",
    "        self.pool5 = nn.MaxPool2d(2, stride=2, ceil_mode=True, return_indices=True) \n",
    "        \n",
    "        # fc1\n",
    "        self.fc6 = CBR(512, 4096, 1, 1, 0)\n",
    "        self.drop6 = nn.Dropout2d()\n",
    "\n",
    "        # fc2\n",
    "        self.fc7 = CBR(4096, 4096, 1, 1, 0)\n",
    "        self.drop7 = nn.Dropout2d()\n",
    "\n",
    "        # Deconv\n",
    "        self.dcb6 = DCB(4096, 512, 7, 1, 3)    \n",
    "        \n",
    "        # Deconv5\n",
    "        self.unpool5 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dcb5_3 = DCB(512, 512, 3, 1, 1)\n",
    "        self.dcb5_2 = DCB(512, 512, 3, 1, 1)\n",
    "        self.dcb5_1 = DCB(512, 512, 3, 1, 1)\n",
    "\n",
    "        # Deconv4\n",
    "        self.unpool4 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dcb4_3 = DCB(512, 512, 3, 1, 1)\n",
    "        self.dcb4_2 = DCB(512, 512, 3, 1, 1)\n",
    "        self.dcb4_1 = DCB(512, 256, 3, 1, 1)\n",
    "        \n",
    "        # Deconv3\n",
    "        self.unpool3 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dcb3_3 = DCB(256, 256, 3, 1, 1)\n",
    "        self.dcb3_2 = DCB(256, 256, 3, 1, 1)\n",
    "        self.dcb3_1 = DCB(256, 128, 3, 1, 1)\n",
    "        \n",
    "        # Deconv2\n",
    "        self.unpool2 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dcb2_2 = DCB(128, 128, 3, 1, 1)\n",
    "        self.dcb2_1 = DCB(128, 64, 3, 1, 1)\n",
    "        \n",
    "        # Deconv1\n",
    "        self.unpool1 = nn.MaxUnpool2d(2, stride=2)\n",
    "        self.dcb1_2 = DCB(64, 64, 3, 1, 1)\n",
    "        self.dcb1_1 = DCB(64, 64, 3, 1, 1)\n",
    "        self.score_fr = nn.Conv2d(64, num_classes, kernel_size = 1)\n",
    "        \n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "                # xavier_uniform은 bias에 대해서는 제공하지 않음 \n",
    "                # ValueError: Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.cbr1_1(x)\n",
    "        h = self.cbr1_2(h)\n",
    "        h, pool1_indices = self.pool1(h)\n",
    "        \n",
    "        h = self.cbr2_1(h)\n",
    "        h = self.cbr2_2(h)\n",
    "        h, pool2_indices = self.pool2(h)\n",
    "        \n",
    "        h = self.cbr3_1(h)\n",
    "        h = self.cbr3_2(h)\n",
    "        h = self.cbr3_3(h)\n",
    "        h, pool3_indices = self.pool3(h)\n",
    "        \n",
    "        h = self.cbr4_1(h)\n",
    "        h = self.cbr4_2(h)\n",
    "        h = self.cbr4_3(h)\n",
    "        h, pool4_indices = self.pool4(h)\n",
    "        \n",
    "        h = self.cbr5_1(h)\n",
    "        h = self.cbr5_2(h)\n",
    "        h = self.cbr5_3(h)\n",
    "        h, pool5_indices = self.pool5(h)\n",
    "        \n",
    "        h = self.fc6(h)\n",
    "        h = self.drop6(h)\n",
    "\n",
    "        h = self.fc7(h)\n",
    "        h = self.drop7(h)\n",
    "        \n",
    "        h = self.dcb6(h)\n",
    "        \n",
    "        h = self.unpool5(h, pool5_indices)\n",
    "        h = self.dcb5_3(h)\n",
    "        h = self.dcb5_2(h)\n",
    "        h = self.dcb5_1(h)\n",
    "        \n",
    "        h = self.unpool4(h, pool4_indices)\n",
    "        h = self.dcb4_3(h)\n",
    "        h = self.dcb4_2(h)\n",
    "        h = self.dcb4_1(h)\n",
    "        \n",
    "        h = self.unpool3(h, pool3_indices)\n",
    "        h = self.dcb3_3(h)\n",
    "        h = self.dcb3_2(h)\n",
    "        h = self.dcb3_1(h)\n",
    "        \n",
    "        h = self.unpool2(h, pool2_indices)\n",
    "        h = self.dcb2_2(h)\n",
    "        h = self.dcb2_1(h)\n",
    "        \n",
    "        h = self.unpool1(h, pool1_indices)\n",
    "        h = self.dcb1_2(h)\n",
    "        h = self.dcb1_1(h)\n",
    "        h = self.score_fr(h)        \n",
    "        return torch.sigmoid(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-02-22T12:14:13.586Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  torch.Size([1, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n",
    "\n",
    "model = DeconvNet(num_classes=21)\n",
    "x = torch.randn([1, 3, 224, 224])\n",
    "print(\"input shape : \", x.shape)\n",
    "out = model(x)\n",
    "print(\"output shape : \", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, validation 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T19:33:20.858721Z",
     "start_time": "2021-01-14T19:33:20.852721Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(num_epochs, model, data_loader, criterion, optimizer, saved_dir, val_every, device):\n",
    "    print('Start training..')\n",
    "    best_loss = 9999999\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, (image, mask) in enumerate(data_loader):\n",
    "            image = image.type(torch.float32)\n",
    "            mask = mask.type(torch.long)\n",
    "            print(image[0].shape)\n",
    "            print('------------')\n",
    "            print(mask[0].shape)     \n",
    "            image, mask = image.to(device), mask.to(device)\n",
    "            \n",
    "            outputs = model(image) \n",
    "            print('------------')\n",
    "            print(outputs[0].shape)\n",
    "            loss = criterion(outputs, mask)\n",
    "            \n",
    "            optimizer.zero_grad()          \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "            if (step + 1) % 25 == 0:\n",
    "                print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(\n",
    "                    epoch+1, num_epochs, step+1, len(train_loader), loss.item()))\n",
    "                \n",
    "        if (epoch + 1) % val_every == 0:\n",
    "            avrg_loss = validation(epoch + 1, model, val_loader, cross_entropy2d, device)\n",
    "            if avrg_loss < best_loss:\n",
    "                print('Best performance at epoch: {}'.format(epoch + 1))\n",
    "                print('Save model in', saved_dir)\n",
    "                best_loss = avrg_loss\n",
    "                save_model(model, saved_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T19:33:21.406317Z",
     "start_time": "2021-01-14T19:33:21.401289Z"
    }
   },
   "outputs": [],
   "source": [
    "def validation(epoch, model, data_loader, criterion, device):\n",
    "    print('Start validation #{}'.format(epoch))\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        cnt = 0\n",
    "        for step, (image, mask) in enumerate(data_loader):\n",
    "            image = image.type(torch.float32)\n",
    "            mask = mask.type(torch.long)\n",
    "            image, mask = image.to(device), mask.to(device)\n",
    "            outputs = model(image)\n",
    "            loss = criterion(outputs, mask)\n",
    "            total_loss += loss\n",
    "            cnt += 1\n",
    "        avrg_loss = total_loss / cnt\n",
    "        print('Validation #{}  Average Loss: {:.4f}'.format(epoch, avrg_loss))\n",
    "   \n",
    "    model.train()\n",
    "    return avrg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 저장 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T19:33:22.441888Z",
     "start_time": "2021-01-14T19:33:22.439387Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(model, saved_dir, file_name='model.pt'):\n",
    "    check_point = {\n",
    "        'net': model.state_dict()\n",
    "    }\n",
    "    output_path = os.path.join(saved_dir, file_name)\n",
    "    torch.save(model, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  모델 생성 및 Loss function, Optimizer 정의\n",
    "\n",
    "- [다중분류를 위한 대표적인 손실함수 : `torch.nn.CrossEntropyLoss`](http://www.gisdeveloper.co.kr/?p=8668)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T19:30:06.778772Z",
     "start_time": "2021-01-14T19:30:06.773273Z"
    }
   },
   "outputs": [],
   "source": [
    "# cross_entropy 동작 원리 : \n",
    "\n",
    "output = torch.Tensor(\n",
    "    [\n",
    "        [0.8982, 0.805, 0.6393, 0.9983, 0.5731, 0.0469, 0.556, 0.1476, 0.8404, 0.5544],\n",
    "        [0.9457, 0.0195, 0.9846, 0.3231, 0.1605, 0.3143, 0.9508, 0.2762, 0.7276, 0.4332]\n",
    "    ]\n",
    ")\n",
    "target = torch.LongTensor([1.4, 5.3])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "loss = criterion(output, target)\n",
    "\n",
    "print(loss) # tensor(2.3519)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T19:33:25.479776Z",
     "start_time": "2021-01-14T19:33:25.475777Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def criterion(input, target, weight=None, size_average=True):\n",
    "    '''\n",
    "    cross_entropy2d\n",
    "    '''\n",
    "    n, c, h, w = input.size()\n",
    "    nt, ht, wt = target.size()\n",
    "\n",
    "    # Handle inconsistent size between input and target\n",
    "    if h != ht and w != wt:  # upsample labels\n",
    "        input = F.interpolate(input, size=(ht, wt), mode=\"bilinear\", align_corners=True)\n",
    "\n",
    "    input = input.transpose(1, 2).transpose(2, 3).contiguous().view(-1, c)\n",
    "    target = torch.LongTensor(target.view(-1))  # target의 type을 lnt로 명시적으로 변환\n",
    "    \n",
    "#     print('input : {}'.format(input.shape))\n",
    "#     print('target : {}'.format(target.shape))\n",
    "#     print('first target index : {}'.format(target[0]))\n",
    "\n",
    "    loss = F.cross_entropy(\n",
    "        input, target, weight=weight, size_average=size_average, ignore_index=250\n",
    "    )\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T19:33:29.965238Z",
     "start_time": "2021-01-14T19:33:26.691740Z"
    }
   },
   "outputs": [],
   "source": [
    "# cross_entropy2d() test\n",
    "model = fcn32(num_classes=22)\n",
    "input = np.transpose(image, [2, 1, 0]).reshape([1, 3, 224, 224])\n",
    "out = model(input)\n",
    "criterion(out,  mask.reshape([1,224,224]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-14T19:33:34.430271Z",
     "start_time": "2021-01-14T19:33:31.691273Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(7777) \n",
    "\n",
    "model = fcn32(num_classes=22)\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.SGD(params = model.parameters(), lr = learning_rate, weight_decay=1e-6)   \n",
    "\n",
    "val_every = 1\n",
    "saved_dir = './saved/FCN32'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-01-14T19:33:35.605Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train(num_epochs, model, train_loader, criterion, optimizer, saved_dir, val_every, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 저장된 model 불러오기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-01-13T07:51:27.414006Z",
     "start_time": "2021-01-13T07:51:27.411008Z"
    }
   },
   "source": [
    "## Reference\n",
    "- [dataloader using VOC2007 Dataset](https://marcinbogdanski.github.io/ai-sketchpad/PyTorchNN/1630_PT_SegNet_VOC2007.html)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "455.097px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
