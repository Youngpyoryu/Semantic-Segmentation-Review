{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T04:17:42.584109Z",
     "start_time": "2021-02-28T04:17:42.578608Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytorch version: 1.4.0\n",
      "GPU 사용 가능 여부: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "import torch\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "from torchvision.models import vgg16\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "print('pytorch version: {}'.format(torch.__version__))\n",
    "print('GPU 사용 가능 여부: {}'.format(torch.cuda.is_available()))\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"   # GPU 사용 가능 여부에 따라 device 정보 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reflected padding 부분\n",
    "\n",
    "> input (image, label)를 각각 reflected padding 진행 후 `front-end`의 input으로 들어감"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T04:17:44.342650Z",
     "start_time": "2021-02-28T04:17:44.327151Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------input------\n",
      "torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[0., 1., 2.],\n",
      "          [3., 4., 5.],\n",
      "          [6., 7., 8.]]]])\n",
      "\n",
      "------after ReflectionPad2d------\n",
      "torch.Size([1, 1, 7, 7])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[8., 7., 6., 7., 8., 7., 6.],\n",
       "          [5., 4., 3., 4., 5., 4., 3.],\n",
       "          [2., 1., 0., 1., 2., 1., 0.],\n",
       "          [5., 4., 3., 4., 5., 4., 3.],\n",
       "          [8., 7., 6., 7., 8., 7., 6.],\n",
       "          [5., 4., 3., 4., 5., 4., 3.],\n",
       "          [2., 1., 0., 1., 2., 1., 0.]]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input image는 전처리 과정에서 input reflected padding 진행\n",
    "\n",
    "input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)\n",
    "print(\"\\n------input------\")\n",
    "print(input.shape)\n",
    "print(input)\n",
    "\n",
    "print(\"\\n------after ReflectionPad2d------\")\n",
    "m = nn.ReflectionPad2d(2)\n",
    "print(m(input).shape)\n",
    "m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T04:17:45.830659Z",
     "start_time": "2021-02-28T04:17:45.827659Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_rfpad_image_label_data(imgae, mask):\n",
    "    rf_pad= nn.ReplicationPad2d(2)\n",
    "    \n",
    "    imgae = rf_pad(imgae)\n",
    "    mask = rf_pad(mask)\n",
    "    \n",
    "    return image, mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:07:24.144030Z",
     "start_time": "2021-02-26T17:07:24.141530Z"
    }
   },
   "source": [
    "### Identity matrix weights intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T04:17:46.887674Z",
     "start_time": "2021-02-28T04:17:46.883673Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 12, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Kernel 가중치 초기화 (일반적)\n",
    "l = nn.Conv2d(12, 20, kernel_size = 3)\n",
    "print(l.weight.size())\n",
    "# l.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T04:17:47.206367Z",
     "start_time": "2021-02-28T04:17:47.202367Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 12, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Kernel 가중치 초기화 (identity matrix weights)\n",
    "l = nn.Conv2d(12, 20, kernel_size = 3)\n",
    "print(l.weight.data.copy_(torch.eye(3)).size())\n",
    "\n",
    "#l.weight.data.copy_(torch.eye(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네트워크 설계 I (Pretrained 된 모델 사용 X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Front-end Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://github.com/choco9966/Semantic-Segmentation-Review/blob/main/Multi-Scale%20Context%20Aggregation%20by%20Dilated%20Convolutions%20(DilatedNet)%20Review/png/front-end%20module.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://github.com/choco9966/Semantic-Segmentation-Review/blob/main/Multi-Scale%20Context%20Aggregation%20by%20Dilated%20Convolutions%20(DilatedNet)%20Review/png/front-end%20module%20with%20context_module.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context Module\n",
    "\n",
    "A context module is constructed based on the dilated convolution as below:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](https://miro.medium.com/max/1576/1*aj0ymQMfAOCXbvhnSlTY_w.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T03:21:07.295469Z",
     "start_time": "2021-02-28T03:21:07.287968Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_context_layers(in_channels, batch_norm=False, version = \"basic\"):\n",
    "    '''\n",
    "    args:\n",
    "     in_channels : input의 channel 수 (e.g. 64)\n",
    "     version : The Context module의 버전 (layer 구성을 위함)\n",
    "    '''\n",
    "    layers = []\n",
    "    \n",
    "    # 1 ~ 7 Layer에 해당되는 dilation 및 channel list \n",
    "    dilation_list = [1, 1, 2, 4, 8, 16, 1]\n",
    "    basic_channels = [1, 1, 1, 1, 1, 1, 1]\n",
    "    large_channels = [2, 2, 4, 8, 16, 32, 32]\n",
    "\n",
    "    \n",
    "    if version == \"basic\":\n",
    "        for i in range(len(basic_channels)):\n",
    "            conv2d = nn.Conv2d(in_channels, in_channels, kernel_size = 3, padding = 1, dilation = dilation_list[i])\n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "        \n",
    "        layers += [nn.Conv2d(in_channels, in_channels, kernel_size = 1, padding = 1, dilation = 1)]\n",
    "        \n",
    "    else :\n",
    "        temp_in_channels = in_channels\n",
    "        for i in range(len(large_channels)):\n",
    "            temp_out_channels = in_channels*large_channels[i]          \n",
    "            conv2d = nn.Conv2d(temp_in_channels, temp_out_channels, kernel_size = 3, padding = 1, dilation = dilation_list[i])\n",
    "            temp_in_channels = temp_out_channels\n",
    "            \n",
    "            if batch_norm:\n",
    "                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]\n",
    "            else:\n",
    "                layers += [conv2d, nn.ReLU(inplace=True)]\n",
    "        \n",
    "        # 8 Layer 해당 \n",
    "        layers += [nn.Conv2d(temp_in_channels, in_channels, kernel_size = 1, padding = 1, dilation = 1)]        \n",
    "    \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-28T03:21:08.321652Z",
     "start_time": "2021-02-28T03:21:08.317121Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(21, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(21, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): Conv2d(21, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(2, 2))\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Conv2d(21, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(4, 4))\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(21, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(8, 8))\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): Conv2d(21, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(16, 16))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(21, 21, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(21, 21, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Context module (basic) \n",
    "make_context_layers(21, batch_norm=False, version = \"basic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:57:31.494367Z",
     "start_time": "2021-02-26T17:57:31.461866Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(21, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(42, 42, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): Conv2d(42, 84, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(2, 2))\n",
       "  (5): ReLU(inplace=True)\n",
       "  (6): Conv2d(84, 168, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(4, 4))\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(168, 336, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(8, 8))\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): Conv2d(336, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), dilation=(16, 16))\n",
       "  (11): ReLU(inplace=True)\n",
       "  (12): Conv2d(672, 672, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (13): ReLU(inplace=True)\n",
       "  (14): Conv2d(672, 21, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The Context module (large)\n",
    "make_context_layers(21, batch_norm=False, version = \"large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T17:08:52.045271Z",
     "start_time": "2021-02-26T17:08:50.779772Z"
    }
   },
   "outputs": [],
   "source": [
    "pretrained_model = vgg16(pretrained = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T01:42:02.347168Z",
     "start_time": "2021-02-27T01:42:02.336168Z"
    }
   },
   "outputs": [],
   "source": [
    "class MultiScaleContextAggregationByDilatedConvolutions(nn.Module):\n",
    "    def __init__(self, num_classes = 21, version = \"basic\"):\n",
    "        super(MultiScaleContextAggregationByDilatedConvolutions, self).__init__()\n",
    "        self.vgg16_model = vgg16(pretrained = False)\n",
    "        features = list(self.vgg16_model.features.children())[0:23]\n",
    "        \n",
    "        '''\n",
    "        100 padding for 2 reasons:\n",
    "            1) support very small input size\n",
    "            2) allow cropping in order to match size of different layers' feature maps\n",
    "        Note that the cropped part corresponds to a part of the 100 padding\n",
    "        Spatial information of different layers' feature maps cannot be align exactly because of cropping, which is bad\n",
    "        '''\n",
    "        features[0].padding = (100, 100)\n",
    "        \n",
    "        \n",
    "        # conv1 (block) ~ conv4 (block)\n",
    "        self.features_map = nn.Sequential(*features)\n",
    "        # conv5 (block) with dilation = 2\n",
    "        self.conv5_block_with_dilation2 = nn.Sequential(nn.Conv2d(512, 512, kernel_size = 3, padding = 1, dilation = 2),\n",
    "                                                        nn.ReLU(inplace=True),\n",
    "                                                        nn.Conv2d(512, 512, kernel_size = 3, padding = 1, dilation = 2),\n",
    "                                                        nn.ReLU(inplace=True),\n",
    "                                                        nn.Conv2d(512, 512, kernel_size = 3, padding = 1, dilation = 2),\n",
    "                                                        nn.ReLU(inplace=True),\n",
    "                                                        )\n",
    "        \n",
    "        # conv6 (block) with dilation = 4\n",
    "        self.conv6_block_with_dilation4 = nn.Sequential(nn.Conv2d(512, 4096, kernel_size = 7, padding = 1, dilation = 4),\n",
    "                                                        nn.ReLU(inplace=True),\n",
    "                                                        nn.Dropout2d(0.5)\n",
    "                                                        )\n",
    "        \n",
    "        # conv7 \n",
    "        self.conv7_blcok = nn.Sequential(nn.Conv2d(4096, 4096,  kernel_size = 1),\n",
    "                                         nn.ReLU(inplace=True),\n",
    "                                         nn.Dropout2d(0.5)                                         \n",
    "                                        )\n",
    "        # conv_final\n",
    "        self.front_end_final_layer= nn.Conv2d(4096, num_classes,  kernel_size = 1)\n",
    "        \n",
    "        \n",
    "        # context module\n",
    "\n",
    "        self.context_module = make_context_layers(num_classes, batch_norm=False, version=version)\n",
    "    \n",
    "         \n",
    "        # Deconvolution (Up) : 불확실\n",
    "        self.deconv = nn.ConvTranspose2d(num_classes, num_classes, kernel_size = 16, stride = 8, padding = 4)\n",
    "        \n",
    "        # Sigmoid \n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # front-end module\n",
    "        x = self.features_map(x)\n",
    "        print('----------------------------------------------')        \n",
    "        print(\"size of after conv4 (block) : {}\".format(x.shape))\n",
    "        x = self.conv5_block_with_dilation2(x)\n",
    "        print(\"size of after conv5_block_with_dilation2 : {}\".format(x.shape))        \n",
    "        x = self.conv6_block_with_dilation4(x)\n",
    "        print(\"size of after conv6_block_with_dilation4 : {}\".format(x.shape))\n",
    "        x = self.conv7_blcok(x)\n",
    "        print(\"size of after conv7_blcok : {}\".format(x.shape))        \n",
    "        x = self.front_end_final_layer(x)\n",
    "        print(\"size of after front_end_final_layer : {}\".format(x.shape))          \n",
    "\n",
    "        # Context module\n",
    "        print('----------------------------------------------')\n",
    "        x = self.context_module(x)\n",
    "        print(\"size of after Context module : {}\".format(x.shape))\n",
    "        \n",
    "        # Deconvolution (Up)\n",
    "        print('----------------------------------------------')\n",
    "        x = self.deconv(x)\n",
    "        print(\"size of after Deconvolution (Up) : {}\".format(x.shape))\n",
    "        print('----------------------------------------------')        \n",
    "        \n",
    "        return self.Sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T01:42:04.897415Z",
     "start_time": "2021-02-27T01:42:03.081884Z"
    }
   },
   "outputs": [],
   "source": [
    "# 구현된 model에 임의의 input을 넣어 output이 잘 나오는지 test\n",
    "\n",
    "model = MultiScaleContextAggregationByDilatedConvolutions(num_classes=22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T01:42:15.994883Z",
     "start_time": "2021-02-27T01:42:04.898885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape :  torch.Size([2, 3, 512, 512])\n",
      "----------------------------------------------\n",
      "size of after conv4 (block) : torch.Size([2, 512, 88, 88])\n",
      "size of after conv5_block_with_dilation2 : torch.Size([2, 512, 86, 86])\n",
      "size of after conv6_block_with_dilation4 : torch.Size([2, 4096, 64, 64])\n",
      "size of after conv7_blcok : torch.Size([2, 4096, 64, 64])\n",
      "size of after front_end_final_layer : torch.Size([2, 22, 64, 64])\n",
      "----------------------------------------------\n",
      "size of after Context module : torch.Size([2, 22, 14, 14])\n",
      "----------------------------------------------\n",
      "size of after Deconvolution (Up) : torch.Size([2, 22, 112, 112])\n",
      "----------------------------------------------\n",
      "output shape :  torch.Size([2, 22, 112, 112])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn([2, 3, 512, 512])\n",
    "print(\"input shape : \", x.shape)\n",
    "out = model(x)\n",
    "print(\"output shape : \", out.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### to do list\n",
    "\n",
    "- `input` 및 `output`을 맞추는 작업 필요\n",
    "- `weigth initialization` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $$S_{out} = \\text{stride} \\ \\times (S_{input}-1) + S_{filter size} - 2 \\ \\times \\ \\text{pad} $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T03:32:55.280803Z",
     "start_time": "2021-02-27T03:32:55.278332Z"
    }
   },
   "outputs": [],
   "source": [
    "stride = 8\n",
    "S_input = 14\n",
    "S_filter = 16\n",
    "pad = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T04:21:17.283707Z",
     "start_time": "2021-02-27T04:21:17.279707Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stride * (S_input - 1) + S_filter - 2*pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T03:33:50.879306Z",
     "start_time": "2021-02-27T03:33:50.780306Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from crfseg import CRF\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Identity(),  # your NN\n",
    "    CRF(n_spatial_dims=2)\n",
    ")\n",
    "\n",
    "batch_size, n_channels, spatial = 10, 3,(100, 100)\n",
    "x = torch.zeros(batch_size, n_channels, *spatial)\n",
    "log_proba = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-27T03:33:54.607998Z",
     "start_time": "2021-02-27T03:33:54.604499Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3, 100, 100])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_proba.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "---\n",
    "\n",
    "- [Dilated Convolution for Semantic Image Segmentation using caffe](https://github.com/fyu/dilation/blob/master/network.py)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "422.591px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
